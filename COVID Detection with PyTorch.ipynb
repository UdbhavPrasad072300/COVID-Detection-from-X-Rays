{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID Detection in X-Rays with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = transforms.Compose([\n",
    "            transforms.ToPILImage()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTrain(Dataset):\n",
    "    def __init__(self):\n",
    "        train_dir = \"xray_dataset_covid19/train/\"\n",
    "        \n",
    "        train_normal_dir = train_dir + \"NORMAL/\"\n",
    "        train_pneumonia_dir = train_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        train_normal_fnames = os.listdir(train_normal_dir)\n",
    "        train_pneumonia_fnames = os.listdir(train_pneumonia_dir)\n",
    "        \n",
    "        self.train_dataset = [[train_normal_dir + image, 0] for image in train_normal_fnames]\n",
    "        self.train_dataset = self.train_dataset + [[train_pneumonia_dir + image, 1] for image in train_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.train_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.train_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTest(Dataset):\n",
    "    def __init__(self):\n",
    "        test_dir = \"xray_dataset_covid19/test/\"\n",
    "        \n",
    "        test_normal_dir = test_dir + \"NORMAL/\"\n",
    "        test_pneumonia_dir = test_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        test_normal_fnames = os.listdir(test_normal_dir)\n",
    "        test_pneumonia_fnames = os.listdir(test_pneumonia_dir)\n",
    "        \n",
    "        self.test_dataset = [[test_normal_dir + image, 0] for image in test_normal_fnames]\n",
    "        self.test_dataset = self.test_dataset + [[test_pneumonia_dir + image, 1] for image in test_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.test_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.test_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = CovidDatasetTrain()\n",
    "covid_trainloader = DataLoader(train_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = CovidDatasetTest()\n",
    "covid_testloader = DataLoader(test_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "torch.Size([2, 1])\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "torch.Size([2, 1])\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.num_flatten = 128 * 3 * 3\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_flatten, 100)\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "    \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = CNN()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 511, 511]             160\n",
      "            Conv2d-2         [-1, 32, 127, 127]           4,640\n",
      "            Conv2d-3           [-1, 64, 31, 31]          18,496\n",
      "            Conv2d-4            [-1, 128, 7, 7]          73,856\n",
      "            Linear-5                  [-1, 100]         115,300\n",
      "            Linear-6                    [-1, 1]             101\n",
      "================================================================\n",
      "Total params: 212,553\n",
      "Trainable params: 212,553\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.00\n",
      "Forward/backward pass size (MB): 36.33\n",
      "Params size (MB): 0.81\n",
      "Estimated Total Size (MB): 41.14\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Udbhav Prasad\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "summary(classifier, input_size=(1, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epoches, loss_hist):    \n",
    "    \n",
    "    for epoch in range(1, n_epoches+1):\n",
    "        model.train()\n",
    "    \n",
    "        epoch_train_loss = 0\n",
    "        epoch_test_loss = 0\n",
    "        \n",
    "        y_true_train = []\n",
    "        y_pred_train = []\n",
    "        \n",
    "        y_true_test = []\n",
    "        y_pred_test = []\n",
    "        \n",
    "        for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.type(torch.FloatTensor).to(device)\n",
    "        \n",
    "            preds = model(data)\n",
    "            \n",
    "            loss = loss_function(preds, label.unsqueeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            y_pred_train.extend(preds.squeeze(1).detach().round().tolist())\n",
    "            y_true_train.extend(label.detach().round().tolist())\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "                \n",
    "            for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "                data = data.to(device)\n",
    "                label = label.type(torch.FloatTensor).to(device)\n",
    "                    \n",
    "                preds = model(data)\n",
    "                \n",
    "                val_loss = loss_function(preds, label.unsqueeze(1))\n",
    "                \n",
    "                y_pred_test.extend(preds.squeeze(1).detach().round().tolist())\n",
    "                y_true_test.extend(label.detach().round().tolist())\n",
    "                \n",
    "                epoch_test_loss += val_loss.item()\n",
    "        \n",
    "        #print(y_pred_test, \"<>\", y_true_test)\n",
    "        \n",
    "        epoch_train_loss = epoch_train_loss / len(covid_trainloader.dataset)\n",
    "        epoch_test_loss = epoch_test_loss / len(covid_testloader.dataset)\n",
    "        \n",
    "        loss_hist[\"train loss\"].append(epoch_train_loss)\n",
    "        loss_hist[\"test loss\"].append(epoch_test_loss)\n",
    "        \n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss / len(covid_trainloader.dataset)))\n",
    "        print(\"       {} Test  mean loss: {:.8f}\".format(epoch, epoch_test_loss / len(covid_testloader.dataset)))\n",
    "        print(\"       Train Accuracy: \", len([True for x, y in zip(y_pred_train, y_true_train) if x==y])/len(y_pred_train), \"==\", len([True for x, y in zip(y_pred_train, y_true_train) if x==y]), \"/\", len(y_pred_train))\n",
    "        print(\"       Test Accuracy: \", len([True for x, y in zip(y_pred_test, y_true_test) if x==y])/len(y_pred_test), \"==\", len([True for x, y in zip(y_pred_test, y_true_test) if x==y]), \"/\", len(y_pred_test))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 1 Train mean loss: 0.00234683\n",
      "       1 Test  mean loss: 0.00865245\n",
      "       Train Accuracy:  0.4594594594594595 == 68 / 148\n",
      "       Test Accuracy:  0.525 == 21 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 2 Train mean loss: 0.00233090\n",
      "       2 Test  mean loss: 0.00869379\n",
      "       Train Accuracy:  0.581081081081081 == 86 / 148\n",
      "       Test Accuracy:  0.5 == 20 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 3 Train mean loss: 0.00222721\n",
      "       3 Test  mean loss: 0.00626089\n",
      "       Train Accuracy:  0.6621621621621622 == 98 / 148\n",
      "       Test Accuracy:  0.8 == 32 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 4 Train mean loss: 0.00150250\n",
      "       4 Test  mean loss: 0.00520798\n",
      "       Train Accuracy:  0.8378378378378378 == 124 / 148\n",
      "       Test Accuracy:  0.8 == 32 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 5 Train mean loss: 0.00169789\n",
      "       5 Test  mean loss: 0.00495003\n",
      "       Train Accuracy:  0.7972972972972973 == 118 / 148\n",
      "       Test Accuracy:  0.8 == 32 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 6 Train mean loss: 0.00142297\n",
      "       6 Test  mean loss: 0.00965910\n",
      "       Train Accuracy:  0.8175675675675675 == 121 / 148\n",
      "       Test Accuracy:  0.675 == 27 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 7 Train mean loss: 0.00119649\n",
      "       7 Test  mean loss: 0.00365011\n",
      "       Train Accuracy:  0.8445945945945946 == 125 / 148\n",
      "       Test Accuracy:  0.875 == 35 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 8 Train mean loss: 0.00115289\n",
      "       8 Test  mean loss: 0.00248121\n",
      "       Train Accuracy:  0.8513513513513513 == 126 / 148\n",
      "       Test Accuracy:  0.975 == 39 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 9 Train mean loss: 0.00089210\n",
      "       9 Test  mean loss: 0.00202721\n",
      "       Train Accuracy:  0.918918918918919 == 136 / 148\n",
      "       Test Accuracy:  0.925 == 37 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 10 Train mean loss: 0.00070900\n",
      "       10 Test  mean loss: 0.00150156\n",
      "       Train Accuracy:  0.9391891891891891 == 139 / 148\n",
      "       Test Accuracy:  0.95 == 38 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 11 Train mean loss: 0.00081616\n",
      "       11 Test  mean loss: 0.00353907\n",
      "       Train Accuracy:  0.9121621621621622 == 135 / 148\n",
      "       Test Accuracy:  0.8 == 32 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 12 Train mean loss: 0.00084675\n",
      "       12 Test  mean loss: 0.00174302\n",
      "       Train Accuracy:  0.8986486486486487 == 133 / 148\n",
      "       Test Accuracy:  1.0 == 40 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 13 Train mean loss: 0.00050717\n",
      "       13 Test  mean loss: 0.00093779\n",
      "       Train Accuracy:  0.9527027027027027 == 141 / 148\n",
      "       Test Accuracy:  1.0 == 40 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 14 Train mean loss: 0.00033262\n",
      "       14 Test  mean loss: 0.00067663\n",
      "       Train Accuracy:  0.9391891891891891 == 139 / 148\n",
      "       Test Accuracy:  1.0 == 40 / 40\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 15 Train mean loss: 0.00026746\n",
      "       15 Test  mean loss: 0.00142133\n",
      "       Train Accuracy:  0.972972972972973 == 144 / 148\n",
      "       Test Accuracy:  0.95 == 38 / 40\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train loss': [0.34733057304008585,\n",
       "  0.3449737151732316,\n",
       "  0.32962671566653895,\n",
       "  0.2223693652572488,\n",
       "  0.25128817995317987,\n",
       "  0.21060011269877874,\n",
       "  0.17708055919344612,\n",
       "  0.17062698062383444,\n",
       "  0.13203019183256892,\n",
       "  0.1049321779959231,\n",
       "  0.1207923631456922,\n",
       "  0.1253194634360576,\n",
       "  0.07506078122177978,\n",
       "  0.04922768263243081,\n",
       "  0.039583988282617145],\n",
       " 'test loss': [0.34609782993793486,\n",
       "  0.34775160253047943,\n",
       "  0.2504356101155281,\n",
       "  0.20831920225173234,\n",
       "  0.1980012516491115,\n",
       "  0.38636414824286475,\n",
       "  0.14600434848107396,\n",
       "  0.09924850985407829,\n",
       "  0.0810885654296726,\n",
       "  0.06006238281843253,\n",
       "  0.14156262227334082,\n",
       "  0.06972090969793499,\n",
       "  0.03751145654023276,\n",
       "  0.027065107051748784,\n",
       "  0.056853231793502344]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist = {}\n",
    "loss_hist[\"train loss\"] = []\n",
    "loss_hist[\"test loss\"] = []\n",
    "\n",
    "NUM_EPOCHES = 15\n",
    "\n",
    "loss_hist = train(classifier, NUM_EPOCHES, loss_hist)\n",
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
