{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID Detection in X-Rays with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import utils\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPIL = transforms.Compose([\n",
    "            transforms.ToPILImage()\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTrain(Dataset):\n",
    "    def __init__(self):\n",
    "        train_dir = \"xray_dataset_covid19/train/\"\n",
    "        \n",
    "        train_normal_dir = train_dir + \"NORMAL/\"\n",
    "        train_pneumonia_dir = train_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        train_normal_fnames = os.listdir(train_normal_dir)\n",
    "        train_pneumonia_fnames = os.listdir(train_pneumonia_dir)\n",
    "        \n",
    "        self.train_dataset = [[train_normal_dir + image, 0] for image in train_normal_fnames]\n",
    "        self.train_dataset = self.train_dataset + [[train_pneumonia_dir + image, 1] for image in train_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.train_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.train_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDatasetTest(Dataset):\n",
    "    def __init__(self):\n",
    "        test_dir = \"xray_dataset_covid19/test/\"\n",
    "        \n",
    "        test_normal_dir = test_dir + \"NORMAL/\"\n",
    "        test_pneumonia_dir = test_dir + \"PNEUMONIA/\"\n",
    "        \n",
    "        test_normal_fnames = os.listdir(test_normal_dir)\n",
    "        test_pneumonia_fnames = os.listdir(test_pneumonia_dir)\n",
    "        \n",
    "        self.test_dataset = [[test_normal_dir + image, 0] for image in test_normal_fnames]\n",
    "        self.test_dataset = self.test_dataset + [[test_pneumonia_dir + image, 1] for image in test_pneumonia_fnames]\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize(1024),\n",
    "            transforms.CenterCrop(1024),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(len(self.test_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.test_dataset[idx]\n",
    "        image = Image.open(data[0])\n",
    "        image = self.transform(image)\n",
    "        return(image, data[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = CovidDatasetTrain()\n",
    "covid_trainloader = DataLoader(train_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = CovidDatasetTest()\n",
    "covid_testloader = DataLoader(test_df, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[0],\n",
      "        [0]])\n",
      "torch.Size([2, 1])\n",
      "148\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1024, 1024])\n",
      "tensor([[1],\n",
      "        [0]])\n",
      "torch.Size([2, 1])\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "    print(data.size())\n",
    "    label = label.unsqueeze(1)\n",
    "    print(label)\n",
    "    print(label.size())\n",
    "    break \n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=2)\n",
    "        \n",
    "        self.num_flatten = 128 * 3 * 3\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.num_flatten, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "    \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        \n",
    "        x = x.view(-1, self.num_flatten)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = CNN()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=1152, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 511, 511]             160\n",
      "            Conv2d-2         [-1, 32, 127, 127]           4,640\n",
      "            Conv2d-3           [-1, 64, 31, 31]          18,496\n",
      "            Conv2d-4            [-1, 128, 7, 7]          73,856\n",
      "            Linear-5                  [-1, 100]         115,300\n",
      "            Linear-6                    [-1, 2]             202\n",
      "================================================================\n",
      "Total params: 212,654\n",
      "Trainable params: 212,654\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.00\n",
      "Forward/backward pass size (MB): 36.33\n",
      "Params size (MB): 0.81\n",
      "Estimated Total Size (MB): 41.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(classifier, input_size=(1, 1024, 1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, out):\n",
    "    labels = torch.round(labels).squeeze(1)\n",
    "    outs = out.squeeze(1)\n",
    "    return(torch.sum(out==labels)/2.0*float(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epoches, loss_hist):    \n",
    "    \n",
    "    for epoch in range(1, n_epoches+1):\n",
    "        model.train()\n",
    "    \n",
    "        epoch_train_loss = 0\n",
    "        epoch_test_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, label, _) in enumerate(covid_trainloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "        \n",
    "            preds = model(data)\n",
    "            #_, preds = torch.max(preds, dim=1)\n",
    "            #preds = preds.type(torch.FloatTensor).to(device)\n",
    "            \n",
    "            #print(preds, \"<>\", preds.size())\n",
    "            #print(label, \"<>\", label.size())\n",
    "            \n",
    "            loss = loss_function(preds, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "                \n",
    "            for batch_idx, (data, label, _) in enumerate(covid_testloader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                    \n",
    "                preds = model(data)\n",
    "                \n",
    "                val_loss = loss_function(preds, label)\n",
    "                epoch_test_loss += val_loss.item()\n",
    "                \n",
    "        epoch_train_loss = epoch_train_loss / len(covid_trainloader.dataset)\n",
    "        epoch_test_loss = epoch_test_loss / len(covid_testloader.dataset)\n",
    "        \n",
    "        loss_hist[\"train loss\"].append(epoch_train_loss)\n",
    "        loss_hist[\"test loss\"].append(epoch_test_loss)\n",
    "        \n",
    "        print(\"-------------------------------------------------\")\n",
    "        print('Epoch: {} Train mean loss: {:.8f}'.format(epoch, epoch_train_loss / len(covid_trainloader.dataset)))\n",
    "        print('       {} Test  mean loss: {:.8f}'.format(epoch, epoch_test_loss / len(covid_testloader.dataset)))\n",
    "        print(\"-------------------------------------------------\")\n",
    "    return loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 1 Train mean loss: 0.00237440\n",
      "       1 Test  mean loss: 0.00870079\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 2 Train mean loss: 0.00235089\n",
      "       2 Test  mean loss: 0.00866564\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 3 Train mean loss: 0.00235720\n",
      "       3 Test  mean loss: 0.00869542\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 4 Train mean loss: 0.00234895\n",
      "       4 Test  mean loss: 0.00866813\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 5 Train mean loss: 0.00220827\n",
      "       5 Test  mean loss: 0.01082747\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 6 Train mean loss: 0.00242235\n",
      "       6 Test  mean loss: 0.00868377\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 7 Train mean loss: 0.00234724\n",
      "       7 Test  mean loss: 0.00865725\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 8 Train mean loss: 0.00234237\n",
      "       8 Test  mean loss: 0.00865521\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 9 Train mean loss: 0.00234801\n",
      "       9 Test  mean loss: 0.00867045\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Epoch: 10 Train mean loss: 0.00234172\n",
      "       10 Test  mean loss: 0.00865803\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train loss': [0.3514111726670652,\n",
       "  0.34793215224871765,\n",
       "  0.3488658437052289,\n",
       "  0.34764471932037455,\n",
       "  0.32682415366021766,\n",
       "  0.35850787404421214,\n",
       "  0.3473909384495503,\n",
       "  0.3466714366867736,\n",
       "  0.34750553603107864,\n",
       "  0.3465749109919007],\n",
       " 'test loss': [0.3480316177010536,\n",
       "  0.34662552624940873,\n",
       "  0.3478167653083801,\n",
       "  0.34672537446022034,\n",
       "  0.4330989237874746,\n",
       "  0.3473509892821312,\n",
       "  0.3462899774312973,\n",
       "  0.346208268404007,\n",
       "  0.3468178525567055,\n",
       "  0.346321240067482]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hist = {}\n",
    "loss_hist[\"train loss\"] = []\n",
    "loss_hist[\"test loss\"] = []\n",
    "\n",
    "loss_hist = train(classifier, 10, loss_hist)\n",
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
